% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads, orivec]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{CALVIS: Chest, wAist and peLVIS circumference from 3D human body meshes	
as ground truth for deep learning}
%
\titlerunning{Chest, wAist and peLVIS circumference from 3D human body meshes}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Yansel Gonzalez Tejeda\orcidID{0000-0003-1002-3815} \and
Helmut Mayer\orcidID{1111-2222-3333-4444}}
%
\authorrunning{Gonzalez-Tejeda and Mayer}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Paris Lodron University of 
	Salzburg, Kapitelgasse 4-65020 Salzburg, Austria
\url{https://www.uni-salzburg.at}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
In this paper we present CALVIS, a method to calculate chest, waist and 
pelvis circumference from 3D human body meshes. Our motivation is to use 
this data as ground truth for learning with convolutional neural networks 
(CNN). Previous work had used the large scale CAESAR dataset or determined 
$\textit{manually}$ these anthropometrical measurements directly from a person 
or human 3D body meshes. 
Unfortunately, acquiring these data is a cost and 
time consuming endeavor. In contrast our method can be used on 
3D meshes automatically. We conduct two experiments. 
In the first experiment we synthesize 10 human body meshes. Then we apply 
CALVIS to calculate chest, waist and pelvis circumference. We evaluate the 
results qualitatively. We observe that the measurements can indeed be used 
to estimate the shape of a person. The second experiment asses the 
plausibility of our approach where we use the calculated human dimensions as 
a ground truth to train a small CNN. After 
having trained the network with 
our data, we demonstrate that learning converges, achieving $x$ percent 
prediction error.
Furthermore, we make the implementation of CALVIS publicly available to 
advance the field.

\keywords{chest, waist and pelvis circumference \and 3D human body mesh \and 
deep learning}
\end{abstract}
%
%
%
\section{Introduction}\label{sec:intro}
\textbf{Motivation} 
Predicting 3D human body measurements from images is crucial in several 
scenarios 
like virtual try-on, animating, ergonomics, computational forensics and 
even health and mortality estimation. Researches had 
named this 
measurements body intuitive controls \cite{Allen.2003}, biometric 
measurements \cite{Sigal.2008}, body dimensions 
\cite{DBLP:conf/bmvc/ChenRC11}, (European standard EN 13402), semantical 
parameters \cite{Yang.2014}, 
traditional anthropometric 
measurements 
\cite{Wuhrer2011} or only ``shape" as in human shape estimation 
\cite{Guan.2013,Bogo:ECCV:2016,Loper.2015,Dibra.2016a,Pishchulin.2017}. In 
contrast, we assume a more 
anthropometric approach 
motivated by comprehensive compendiums like Panero and Zelnik, 1979 
\cite{panero1979human}.
Throughout this paper the term human body 
dimensions 
will be used to refer to the above measurements and we will focus on three of 
these dimensions, namely chest, waist and 
pelvis circumference.

The problem of estimating chest, waist and 
pelvis circumference having only an image of a person is a challenging problem 
and involve recognizing the corresponding location of these dimension in the 
body and estimating their circumference. Additionally, the setting is an  
under-constrained  (or inverse) problem. Information 
gets lost when a camera is 
used to capture the human body in 3D space to 'render' a 2D image. 

To tackle this problem a supervised learning approach can be used. This approach
demands large amount of human body anthropometric measurements and is certainly 
one of the biggest challenges in the field.  Currently there 
is only one large-scale dataset, the Civilian American and 
European Surface Anthropometry Resource (CAESAR) \cite{robinette1999caesar} 
with 3D human body scans and their corresponding anthropometric measurements. 
This survey was extraordinarily large and resource intensive: around nine 
thousand people from Europe and the USA where scanned, it was conducted over 
more than five years and costed more than six million dollars.

In the past decade a noticeably amount of researchers have employed this 
dataset to investigate human shape estimation. Because the measurement 
acquiring process is resource intensive and requires large amount of human and 
material resources, this type of studies are rare and the data they offer is 
expensive. Therefore, it is important to explore alternative methods where 
human body measurements derived from real data can be obtained for 
investigation.

3D human body generative models offers such an alternative. We propose to 
synthesize 3D human body meshes using the SMPL \cite{Loper.2015} generative 
model. Once we have 
the 3D meshes we can compute chest, waist and pelvis circumference. The next 
step after obtaining the 
measurements is to use a camera model to render
images. Finally, in possession of this ground truth we can 
input this images to the learning algorithm and perform inference.

\textbf{Contributions} In summary, our contributions are $1)$ CALVIS: a method 
capable of automatically outputting chest, waist and pelvis circumference from 
3D body meshes. $2)$ A prototype deep learning framework (CNN) with synthetic 
images of 3D human body meshes as input and the human body dimension obtained 
with CALVIS as supervision signal.

\section{Related Work}\label{sec:rel_work}

\subsection{Human Body Data synthesis}
The \cite{Anguelov.2005} SCAPE model opened wide possibilities in the field  
because it provided the community with a generative model of the human 
body. However, if (Problem they address) (which method?)(Results?) (Open 
Problems).
After some(which, reference here) attempts on improving (say here either 
quality or spped), Loper et al., 2018 \cite{Loper.2015} developed a generative 
human body model.(Problem they address) (which method?)(Results?) (Open 
Problems).
Pischchulin et al., 2017 \cite{Pishchulin.2017} synthesize data. (Problem they 
address) (which method?)(Results?) (Open Problems).
More recently, Joo et al. \cite{Joo.2018} presents a human model with SMPL as a 
body model and added hand and face. We do not use that more complex model in 
this paper, that is future work.

\subsection{Human Body Dimensions from 3D models}
Allen et al., 2003 \cite{Allen.2003} proposed how to relate what they call body 
intuitive controls, e.g. height, weight, age and sex with PCAs performed on the 
space X (replace X!) using a linear mapping. Describe more please! (Problems)- 
make connection to next related work.  
Sigal et al., 2008 \cite{Sigal.2008} used a generative model (which method?) to 
estimate what he calls biometric measurements. (Results?) (Open Problems).
It is obvious that the problem of estimating the shape and pose of a human 
having only an image it is an under-constrained  (or inverse) problem. 
\cite{Guan.2013}  argues that (why) this situation can be alleviated by 
providing the shape model with the capability of keeping the height constant. 
This is indeed not the case for the original SCAPE model because the height is 
a semantical parameter and semantical parameters are “spread” over all bases. 
The arguments goes like this: SCAPE is a good human shape model. At its core 
this model relates triangles deformations to semantical parameters. One of the 
first problem one encounters when trying to model the human shape is the model 
size: because one needs a lot (~6000) triangles to sculpt a human body, you 
have the same amount of dimensions. Then one applies dimensionality reduction 
techniques, in this case PCA, having at the end around 20 components. These 
components do not describe intuitively (are not expressed explicit in the model 
) human shape characteristics (e.g., height, chest circumference or waist), 
nevertheless they are the bases of a vector space containing human shapes. Next 
\cite{Guan.2013} goes an argue that by “rotating” one (or more) of these bases 
we can vary the other bases  without being forced to vary the height.

Wuhrer and Shu, 2013 \cite{Wuhrer.2013} proposed a method to 
create/estimate/generate 3D human shapes from traditional anthropometric 
measurements. They learned a correlation between the 3D bodies meshes and 1D 
measurements. Then, when predicting a new shape, they find an initial solution 
based on the learned correlation (yansel WHICH correlation) and refine this 
solution by a two-step non linear optimization. They build up on two 
assumptions, namely, that the initial estimation should be in the plausible 
space of human shapes and second, that, in order to generalized, the model has 
to be refined without using this prior knowledge. While the first step use 
information from the learned shape prior, the second step introduce a 
smoothness constraint on the objective function. Like this work we use a set of 
traditional anthropometric measurements, unlike them we calculate 1D 
measurements from 3D human bodies to use them as ground truth for later 
inference. Their approach is able to capture variation outside the shape space 
spanned by the training data, in other words, the approach generalized well.
At the same time, Boisvert et al., 2013 \cite{Boisvert.2013} estimate human 
shape from measurements (what is that)(which method?)(Results?) (Open 
Problems). These two works can not be directly quantitatively compared because 
they use different set of measurements.
Moreover, Yang et al., 2014 \cite{Yang.2014} developed a method to map 
principal component to what they call semantical parameters. These semantical 
parameter are indeed anthropometric measurements, which justify their naming: 
in contrast to anthropometric meaningless principal components, their model is 
capable of manipulating concepts like chest and waist 
circumference.(Method?)(Results?).   
Tsoli et al., 2014 \cite{Tsoli.2014} extract what they calls anthropometric or 
tailored measurements introduced model-based anthropometry (what is that)(which 
method?)(Results?) (Open Problems).

\subsection{Human Shape Estimation with Artificial Neural Networks}
A huge amount of research has been conducted in recent years to address the 
problem of 3D/2D human shape estimation using ANNs. Experimented with different 
architectures.
An state-of-the-art method is \cite{kanazawaHMR18} where they estimate human 
shape and pose.

%------------------------------------------------------------------------
\section{Approach}\label{sec:approach}
While human body 
meshes are traditionally acquired by specialized 3D scanners, we use a 
generative model to synthesize them, which we briefly review in 
subsection \ref{subsec:smpl_model}. To calculate the actual human body 
dimensions, we first need to formalize the 
notion of chest, waist and pelvis of a 3D human body mesh. Our strategy consist 
of segmentating the mesh in five regions, 
three of them of interest, which we 
discuss in subsection \ref{subsec:three_regions}. Finally, within these regions 
we identify the intended dimensions employing a human body mesh signature 
(HBMS), 
defined in subsection \ref{subsec:hbm_signature}.

\subsection{SMPL Generative Model of the Human Body}\label{subsec:smpl_model}
In this work we synthesize 3D human meshes using the SMPL
model \cite{Loper.2015}. The generative nature of our approach establishes this 
model as starting point (and not the 3D mesh). Nevertheless, our method is 
flexible enough to begin the pipeline with a 3D mesh. In that case, an SMPL 
model can be fitted to the volumetric data, using the method described by 
Varol et al., 2018 \cite{varol18_bodynet} for example.

The SMPL model is at its top level a \textbf{skinned articulated 
model}, i.e., 
consists of a 
surface mesh $\mathcal{M}$ that mimics the skin and a corresponding skeleton 
$\mathbf{J}$. The mesh $\mathcal{M} \in \mathbb{R}^{3N}$, which is a boundary 
representation stores 
both the body geometry (vertex position) and topology (how the vertices are 
connected). The skeleton $\mathbf{J}$ is defined by its joints location in 3D 
space $j_i \in \mathbb{R}^3$  and its kinematic tree. Two 
connected 
joints define a `bone'. Moreover, a child bone is rotated relative to 
its 
connected parent. The pose $\vec{\theta}$ is the specification of every bone 
rotation plus an orientation for the root bone.

The SMPL model is also a \textbf{deformable model} 
\cite{Terzopoulos.1987}. In order to 
synthesize a 
new human mesh one has to deform the provided template mesh by 
setting shape parameters $\vec{\beta}$. Pose parameters $\vec{\theta}$ are used 
for animation.

More specifically, the model is defined by a template mesh (mean template 
shape) $\mathbf{\bar{T}}$ represented by a vector of $N = 6890$ concatenated 
vertices in the zero pose, $\vec{\theta}^*$. The skeleton has $K = 23$ joints.

As an \textbf{statistical model}, SMPL provides learned parameters
\begin{equation} \label{eq:smpl_params}
\Phi = \{\mathbf{\bar{T}}, \mathcal{W}, \mathcal{S}, \mathcal{J}, 
\mathcal{P}\}.
\end{equation}

As mentioned above $\mathbf{\bar{T}} \in \mathbb{R}^{3N}$  is the mean template 
shape. The set of blend weights $\mathcal{W} \in \mathbb{R}^{N \times K}$ 
represents 
how much the rotation of 
skeleton bones affects
the vertices. In addition, the matrices $\mathcal{S}$ and $\mathcal{P}$ define 
respectively linear functions $B_s(\vec{\beta}; \mathcal{S}): 
\mathbb{R}^{|\vec{\beta}|} \to 
\mathbb{R}^{3N}$ and $B_p(\vec{\theta}; \mathcal{P}): 
\mathbb{R}^{|\vec{\theta}|} \to 
\mathbb{R}^{3N}$ that are used to deform $\mathbf{\bar{T}}$; and the function 
$\mathcal{J}: \mathbb{R}^{|\vec{\beta}|} \to \mathbb{R}^{3K}$ predicts skeleton 
rest joint locations from vertices in the rest 
pose. A new mesh $\mathcal{M}_{new}$ can be then generated

\begin{equation}\label{eq:gen_mesh}
\mathcal{M}_{new} = M(\vec{\beta}, \vec{\theta}, \Phi).
\end{equation}

Since we held fix parameters $\Phi$ during the synthesis and either we change 
$\vec{\theta}$ because our approach focuses on the zero pose $\vec{\theta}^*$, 
we manufacture a new mesh

\begin{equation}\label{eq:gen_mesh_only_shape}
\mathcal{M}_{new} = M(\vec{\beta}).
\end{equation}

\begin{equation}\label{eq:gen_new_mesh}
\mathcal{M}_{new} = \mathbf{\bar{T}} + B_s(\vec{\beta}; \mathcal{S}).
\end{equation}

As a result we obtain a mesh that realistically represents a human body. Next, 
we focus on establishing reasonable chest, waist and pelvis regions on this 
mesh.

\subsection{Chest, Waist and Pelvic Regions 
Segmentation}\label{subsec:three_regions}

Let us consider a human body mesh $\mathcal{M}$. Our method requires that 
$\mathcal{M}$ is standing with arms raised 
parallel to the 
ground at shoulder height at a $90^\circ$ angle (SMPL normalized pose
$\vec{\theta}^*$). Additionally, we assume that the mesh 
has LSA orientation, e.g., x, y and z axis are positively directed from 
right-to-left, inferior-to-superior and posterior-to-anterior, respectively. If 
the mesh has another orientation we can always apply transformations to 
LSA-align it.

\begin{figure}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{subjects_2_6_calvis_segmented.png}
	\end{center}
	\caption{Mesh segmentation in chest (red), waist (green) and pelvic (blue) 
	regions. We show male (left) and female (right) subjects in frontal and 
	lateral view. Note how the regions are correctly segmented for different 
	gender and body shapes.}
	\label{fig:mesh_segmented}
\end{figure}

We observe that the chest circumference is usually measure bellow the armpits, 
also known as axilla, but above the natural waist. Similarly, the pelvis 
circumference is measured often around the hips. This observation suggests that 
we should consider segmenting the mesh in meaningful regions. Moreover, we can 
use the skeleton when determining the region boundaries. For 
example, there is consensus regarding the axilla 
definition as the area of the human body underneath the shoulder joint 
(\cite{MeSH.axilla}, \cite{FMA.axilla}, \cite{TA.axilla}). Therefore, we can 
use the shoulder joint as a hint to locate the axilla and establish a upper 
bound for the chest region. More generally, we can use the 
skeleton joints defined in $\mathbf{J}$ to identify the regions boundaries.

Let the axilla location be the point $p_a = (p^x_a, p^y_a, p^z_a)$ and 
$j_{nw}$, $j_{pl}$, 
$j_{lh} \in \mathbf{J}$ be the natural waist, pelvis and left hip joints, 
respectively.

Recall that mesh $\mathcal{M}$ has $N$ vertices. Let the set of vertices be 
$\mathcal{V}$ and $v^y_i$ the y-coordinate of vertex $v_i$, we can 
segment the 
mesh in chest, waist and pelvic regions 
$\mathcal{CR}, \mathcal{WR}, \mathcal{PR}$, respectively

\begin{align}\label{eq:cr}
\mathcal{CR} = \big(v^y_i \, | \, p^y_a > v^y_i \geq j^y_{nw} \big)
\end{align}

\begin{align}\label{eq:wr}
\mathcal{WR} = \big(v^y_i \, | \, j^y_{nw} > v^y_i \geq j^y_{pl}\big)
\end{align}

\begin{align}\label{eq:pr}
\mathcal{PR} = \big(v^y_i \, | \, j^y_{pl} > v^y_i \geq j^y_{lh}\big)
\end{align}

Figure \ref{fig:mesh_segmented} shows the result of applying these equations on 
two meshes. Note how the regions are correctly segmented for different gender 
and body shapes. For example, the male subject on the left is smaller than the 
female subject on the right. Additionally, the subjects have very different 
muscular and fat build.

\subsubsection{Axilla recognition}\label{subsec:armpit_recog}
As mentioned above, we would like to measure the chest circumference below the 
armpit, also known as axilla. This raises the question whether we can recognize 
the axilla. What we want is a point $p_a$ in the mesh at the proper location 
under 
the arms. One point suffices because we require only a reference $p^y_a$ along 
the 
y-axis to define the chest region according to equation \ref{eq:cr}. Therefore, 
we focus on the left shoulder joint $j_{ls} \in \mathbf{J}$. Since the axilla 
is related to 
the shoulder joint, we can cast a ray $\boldsymbol{\iota}$ from it in direction 
to the middle left 
inferior edge of the bounding box outside the mesh at point

\begin{align}
p_b = \bigg(v^x_{min}, v^y_{min}, v^z_{min} + 
\frac{v^z_{max} - v^z_{min}}{2}\bigg).
\end{align}

Here $v^x_{min}, v^y_{min}, v^z_{min}$ are vertex minimum $x$,$y$ and $z$ 
coordinates, respectively; analog is $v^z_{max}$ the maximum vertex $z$ 
coordinate. The ray 
$\iota$ is then determined by points $j_{ls}$ (inside the mesh) and $p_b$ 
(outside the mesh). Therefore, the ray $\iota$ intersects the mesh at one point 
$p_a$ (Figure \ref{fig:axilla-recognition}), which we consider the axilla 
center.

Once we have a segmented mesh, we search the dimensions within 
these regions. The body dimensions we want to calculate are 
geodesic on the mesh. Next subsection discuss how.

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{axilla_recognition_final.png}
	\end{center}
	\caption{Axilla recognition based on the skeleton joint. We cast a ray from 
	the left shoulder joint $j_{ls}$ in direction to the middle left 
		inferior edge of the bounding box (left). The ray intersects the mesh 
		at point $p_a$ that we consider the axilla center. We plot the 10 
		nearest vertices to $p_a$ (right). As it can be observed, the axilla is 
		properly recognized.}
	\label{fig:axilla-recognition}
\end{figure}

\subsection{Human Body Mesh Signature}\label{subsec:hbm_signature}
Intuitively, we would like to measure the chest circumference below the arms 
at the \textit{widest} part of the torso and the waist circumference at the 
\textit{narrowest} part beneath the chest but above the hips. This intuition is 
compliant 
with prior body dimensions standardized definition, for example, the European 
standard EN 13402-1 \cite{en13402-1}. Similarly, the pelvis 
circumference is measured often around the \textit{widest} part of hips and 
buttocks. The general idea is to cast the chest, waist and pelvis circumference 
estimation problem as an optimization problem. If we are able to establish a 
function that measures geodesics on the mesh, we can search for its maximum, 
e.g., chest in the chest region.

We 
can formalize this intuition by considering the cross-sectional length of the 
2D intersection curves\cite{book.compu.topo} along the y-axis. Moreover, we 
can 
use a plane $\boldsymbol{\pi_j}$ parallel (with normal $\vec{n}_{||}$) to the 
floor to intersect the mesh at point $q_j = (0, q^y_j, 0), \,  q^y_j \in 
\mathbb{R}$ along the y-axis. Since $\mathcal{M}$ is triangulated, the boundary 
of this 
intersection is a collection of segments $s_i$. Therefore, we can 
determine the intersection boundary length as
\begin{equation}\label{eq:boundary_length}
\mathcal{BL}(\mathcal{M}, \vec{n}_{||}, q) = \sum_{i = 
	1}^{i = N}s_i
\end{equation}

Note that for cross sections where $\boldsymbol{\pi_j}$ intersects the legs, 
two 
disconnected curves will appear. This is not a problem, because a 
set of disconnected curves is, in our case, a collection of segments with 
non-zero length.

\begin{figure}[h!]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=0.5\linewidth]{subject_2_with_cutting_planes_xrays.png}
	\end{center}
	\caption{3D mesh iterative slicing. We 
		can use a plane $\boldsymbol{\pi_j}$ parallel (with normal 
		$\vec{n}_{||}$) to the 
		floor to intersect the mesh at point $q_j = (0, q^y_j, 0), \,  q^y_j 
		\in 
		\mathbb{R}$ along the y-axis. By varying point $q_j$ (we show here 40), 
		we can obtain 
		slices.}
	\label{fig:subjects_planes_xrays}
\end{figure}

Starting 
from the 
top $q_t$ of the bounding box we slice iteratively mesh $\mathcal{M}$ with 
plane 
$\boldsymbol{\pi_j}$ every $m$-meters along the y-axis until we reach the 
bounding box bottom 
$q_b$. Figure \ref{fig:subjects_planes_xrays} shows 40 of these 
slides. Next, we assemble the mesh sliding vector $\vec{\mathcal{L}}$. The 
elements of 
this vector are slice point y-coordinates 

\begin{align}\label{eq:mesh_slice_vector}
\vec{\mathcal{L}}(\mathcal{M}, \vec{n}_{||}, m) = \left[ q_t, q_t+m, \cdots, 
q_b \right]
\end{align}

Finally, we apply equation \ref{eq:boundary_length} to every element of 
$\mathcal{L}$ in equation \ref{eq:mesh_slice_vector}. In other words, for every 
slice at point 
$q_j$, we compute $\mathcal{BL}(q_j)$. Here we drop from the notation mesh 
$\mathcal{M}$ and plane normal $\vec{n}_{||}$ because they remain constant. We 
then can 
define the \textbf{human body mesh signature} $\mathcal{MS}: 
\vec{\mathcal{L}} \to \mathbb{R}$ that maps every slice at point $q_j$ to the 
corresponding boundary length $\mathcal{BL}(q_j)$.

\begin{align}\label{eq:mesh_signature_short}
\mathcal{MS}(\vec{\mathcal{L}}) = [\mathcal{BL}(\mathcal{L}_1), 
\mathcal{BL}(\mathcal{L}_2), \cdots, 
\mathcal{BL}(\mathcal{L}_{|\vec{\mathcal{L}}|})]
\end{align}

\begin{align}\label{eq:mesh_signature}
\mathcal{MS}(\mathcal{M}, \vec{n}_{||}, m) = 
[\mathcal{BL}(q_t), \mathcal{BL}(q_t+m), \cdots, \mathcal{BL}(q_b)]
\end{align}

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{Figure_1.eps}
	\end{center}
	\caption{Human Body Mesh Signature for male and female meshes. The 
		function resembles a rotated silhouette of the human body and exhibits 
		several \textit{extrema}. See section 
		\ref{subsec:mesh_signature_extrema} for discussion of these extrema.}
	\label{fig:hbm_signature}
\end{figure}

Figure \ref{fig:hbm_signature} shows the $\mathcal{MS}$ of two meshes (male 
and female) for 
$m=0,001$ and plane parallel to the floor (with normal 
$\vec{n}_{||} = (0, 
1,0)$) using the library trimesh \cite{trimesh}. Note that the function as 
defined by equation \ref{eq:mesh_signature} is bounded and not continuous. It 
resembles a rotated silhouette of the human body and exhibits several 
\textit{extrema}.

In general, we expect these \textit{extrema} to be adequate features to 
calculate the human dimensions. More specifically, we assume that:
\begin{enumerate}
	\item The chest circumference $cc$ is the local maximum within the chest 
	region.
	\item The waist circumference $wc$ (not to be confused with the natural 
	waist line) is the minimum within the waist region.	
	\item The pelvis circumference $pc$ is the maximum within the pelvis region.
\end{enumerate}

\begin{align}\label{eq:cc}
cc &= \mathcal{L}_i \in \mathcal{CR}\\
\mathcal{MS}(\mathcal{M}, \vec{n}_{||}, m) &= \mathcal{L}_i \in \mathcal{CR}
\end{align}

%------------------------------------------------------------------------
\section{Experiments and Results}

We conduct two experiments. In the first experiment we synthesize eight (four 
female and four male) human body meshes using shape parameters provided by 
SURREAL \cite{varol17_surreal}.
The meshes reflect human figures characteristics such as bulky, slim, small and 
tall subjects. Then we apply CALVIS to calculate chest, waist and pelvis 
circumference. Since we do not have ground truth, we evaluate the results 
qualitatively by comparing our method 
with \cite{Dibra.2016b}.
The second 
experiment serves to asses the plausibility of our approach to use the 
synthetic data for deep learning. We input the calculated human 
dimensions to an artificial neural network. After having trained the network 
with our data, we show that learning converges.

\subsection{Qualitative evaluation}\label{subsec:qualitative_eval}
Here we evaluate our method qualitatively with method one and method two.
\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{subject_6_with_cutting_planes_on_joints.eps}
	\end{center}
	\caption{Qualitative evaluation for male and female meshes. We evaluate our 
	method with method one and method two.}
	\label{fig:qualitative_eval}
\end{figure}

Figure \ref{fig:qualitative_eval} shows the $\mathcal{MS}$ of two meshes (male 
and female) for 
$m=0,001$ and plane $\boldsymbol{\pi}$ parallel to the floor (with normal $(0, 
1,0)$) using the library trimesh \cite{trimesh}. Note that the function as 
defined by equation \ref{eq:mesh_signature} is bounded and not continuous. It 
resembles a rotated silhouette of the human body and exhibits several 
\textit{extrema}.

\begin{figure}[h!]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{Figure_1.eps}
	\end{center}
	\caption{Human Body Mesh Signature for male and female meshes. The 
		function resembles a rotated silhouette of the human body and exhibits 
		several \textit{extrema}. See section 
		\ref{subsec:mesh_signature_extrema} for discussion of these extrema.}
	\label{fig:qualitative_eval}
\end{figure}

\subsection{Learning convergence evaluation}\label{subsec:learn_conv}
This experiment demonstrates the plausibility of using calvis to obtain data 
that can be used as a ground truth for machine learning (in general) and 
specifically as supervising signal for ANN.

Figure \ref{fig:calvis_net} shows the our prototypical ANN calvis net where the 
input is a synthetic image from a 3D mesh and the output are the three body 
dimensions.

Figure \ref{fig:learn_conv} shows that after epoch number 5 the estimation 
error decreases and the learning converges.

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{Figure_1.eps}
	\end{center}
	\caption{Human Body Mesh Signature for male and female meshes. The 
		function resembles a rotated silhouette of the human body and exhibits 
		several \textit{extrema}. See section 
		\ref{subsec:mesh_signature_extrema} for discussion of these extrema.}
	\label{fig:qualitative_eval}
\end{figure}

\begin{figure}[H]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
		\includegraphics[width=\linewidth]{Figure_1.eps}
	\end{center}
	\caption{Human Body Mesh Signature for male and female meshes. The 
		function resembles a rotated silhouette of the human body and exhibits 
		several \textit{extrema}. See section 
		\ref{subsec:mesh_signature_extrema} for discussion of these extrema.}
	\label{fig:qualitative_eval}
\end{figure}


%------------------------------------------------------------------------
\section{Conclusion}

The method can be optimized. Further research must be conducted.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{egbib}
%

\end{document}
